= Pytest

==Installation
Pytest is available for installation from the https://pypi.python.org/pypi/allure-pytest[PyPI], therefore
installation with pip is recommended. To install the latest version, execute from the command line:

[source, bash]
----
$ pip install allure-pytest
----

That will install allure-pytest and allure-python-commons packages to produce report data compatible with Allure 2.
If you are using a previous version of adapter for the
https://pypi.python.org/pypi/pytest-allure-adaptor[first generation] of Allure reports then you will need to
uninstall it first.

==Usage
To enable Allure listener to collect results during the test execution simply add `--alluredir` option and
provide path to the folder where results should be stored. E.g.:

[source, bash]
----
$ pytest --alluredir=/tmp/my_allure_results
----

To see the actual report after your tests have finished, you need to use Allure commandline utility to generate report
 from the results.

[source, bash]
----
$ allure serve /tmp/my_allure_results
----

This command will show you generated report in your default browser.

== Basic Reporting

Your can see all default pytest statuses in the Allure report: only tests that were not succeeded due to one of the
assertion errors will be marked as failed, any other exception will cause a test to have a broken status.

[source, python]
----
import pytest

def test_success():
    """this test succeeds"""
    assert True


def test_failure():
    """this test fails"""
    assert False


def test_skip():
    """this test is skipped"""
    pytest.skip('for a reason!')


def test_broken():
    raise Exception('oops')
----

== Supported Pytest features

Some of the common Pytest features that the Allure report supports include xfails, fixtures and
finalizers, marks, conditional skips and parametrization.

=== Xfail

This is pytest way of marking expected failures: (https://docs.pytest.org/en/latest/skipping.html[Pytest docs])

[source, python]
----
@pytest.mark.xfail(condition=lambda: True, reason='this test is expecting failure')
def test_xfail_expected_failure():
    """this test is an xfail that will be marked as expected failure"""
    assert False


@pytest.mark.xfail(condition=lambda: True, reason='this test is expecting failure')
def test_xfail_unexpected_pass():
    """this test is an xfail that will be marked as unexpected success"""
    assert True
----

Which results in test being skipped and marked with a special tag when it is expected to fail.
image::pytest_xpass_expected_failure.png["Expected xpass failure"]

And special marking in description and a special tag when it unexpectedly passed.
image::pytest_xpass_unexpected_pass.png["Unexpected xpass pass"]

=== Conditional mark

In Pytest you can conditionally mark a test to not be executed under some specific conditions
 (https://docs.pytest.org/en/latest/skipping.html[Pytest docs]):

[source, python]
----
@pytest.mark.skipif('2 + 2 != 5', reason='This test is skipped by a triggered condition in @pytest.mark.skipif')
def test_skip_by_triggered_condition():
    pass
----

When condition is evaluated to true, test receives a 'Skipped' status in report, a tag and a description from the
annotation.
image::pytest_conditional_skip.png["Conditional skip triggered"]

=== Fixtures and Finalizers

Fixtures and finalizers are the utility functions that will be invoked by Pytest before your test starts and after your
test ends respectively. Allure tracks invocations of every fixture and shows in full details what methods with what
arguments were invoked, preserving the correct sequence of the calls that were made.
 (https://docs.pytest.org/en/latest/reference.html#id30[Pytest docs])

You don't need to mark your fixtures to make them visible in the report, they will be detected automatically for
different skopes.

[source, python]
----
@pytest.fixture(params=[True, False], ids=['param_true', 'param_false'])
def function_scope_fixture_with_finalizer(request):
    if request.param:
        print('True')
    else:
        print('False')
    def function_scope_finalizer():
        function_scope_step()
    request.addfinalizer(function_scope_finalizer)


@pytest.fixture(scope='class')
def class_scope_fixture_with_finalizer(request):
    def class_finalizer_fixture():
        class_scope_step()
    request.addfinalizer(class_finalizer_fixture)


@pytest.fixture(scope='module')
def module_scope_fixture_with_finalizer(request):
    def module_finalizer_fixture():
        module_scope_step()
    request.addfinalizer(module_finalizer_fixture)


@pytest.fixture(scope='session')
def session_scope_fixture_with_finalizer(request):
    def session_finalizer_fixture():
        session_scope_step()
    request.addfinalizer(session_finalizer_fixture)


class TestClass(object):

    def test_with_scoped_finalizers(self,
                                    function_scope_fixture_with_finalizer,
                                    class_scope_fixture_with_finalizer,
                                    module_scope_fixture_with_finalizer,
                                    session_scope_fixture_with_finalizer):
        step_inside_test_body()
----

image::pytest_skoped_finalizers.png["Test with fixtures and finalizers executed within different scopes."]

Depending on an outcome of a fixture execution, test that is dependent on it may receive a different status.
Exception in the fixture would make all dependent tests broken, `pytest.skip()` call would make all dependent test
skipped.

[source, python]
----
import pytest

@pytest.fixture
def skip_fixture():
    pytest.skip()


@pytest.fixture
def fail_fixture():
    assert False


@pytest.fixture
def broken_fixture():
    raise Exception("Sorry, it's broken.")


def test_with_pytest_skip_in_the_fixture(skip_fixture):
    pass


def test_with_failure_in_the_fixture(fail_fixture):
    pass


def test_with_broken_fixture(broken_fixture):
    pass
----

image::pytest_fixture_effect.png["Fixture execution outcome resulting in different statuses."]

=== Parametrization

You can generate many test cases from the sets of input parameters using `@pytest.mark.parametrize`.
 (https://docs.pytest.org/en/latest/skipping.html[Pytest docs])

All argument names and values will be captured in the report, optionally argument names will be replaced with
provided string descriptions in the `ids` kwarg.

[source, python]
----
import allure
import pytest


@allure.step
def simple_step(step_param1, step_param2 = None):
    pass


@pytest.mark.parametrize('param1', [True, False], ids=['id explaining value 1', 'id explaining value 2'])
def test_parameterize_with_id(param1):
    simple_step(param1)


@pytest.mark.parametrize('param1', [True, False])
@pytest.mark.parametrize('param2', ['value 1', 'value 2'])
def test_parametrize_with_two_parameters(param1, param2):
    simple_step(param1, param2)


@pytest.mark.parametrize('param1', [True], ids=['boolean parameter id'])
@pytest.mark.parametrize('param2', ['value 1', 'value 2'])
@pytest.mark.parametrize('param3', [1])
def test_parameterize_with_uneven_value_sets(param1, param2, param3):
    simple_step(param1, param3)
    simple_step(param2)
----

Example of captured test invocations with different sets of named and unnamed parameters.
image::pytest_parameterized_tests.png["Multiple invocations of tests with different parameters."]

Details of test execution for a parameterized test with a named parameter.
image::pytest_parameterized_with_id.png["Multiple invocations of tests with different parameters."]